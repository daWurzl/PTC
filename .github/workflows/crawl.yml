name: zeitsteuerung

on:
  schedule:
    - cron: "0 */4 * * *"   # Läuft alle 4 Stunden um :00 Uhr UTC
  workflow_dispatch:        # Manuelle Auslösung über GitHub UI

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 90     # Erhöhtes Timeout für längere Crawls

    steps:
      # 1. Repository-Code herunterladen
      - name: Checkout code
        uses: actions/checkout@v4

      # 2. Python 3.11 einrichten
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # 3. Installiere benötigte Pakete (mit uvloop)
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp beautifulsoup4 pydantic python-dotenv playwright uvloop

      # 4. Playwright-Browser installieren
      - name: Install browsers
        run: playwright install chromium

      # 5. Datenverzeichnis erstellen
      - name: Create directories
        run: |
          mkdir -p data
          echo "Directory structure:" && tree

      # 6. Crawler ausführen
      - name: Run crawler
        env:
          OUTPUT_CSV: data/results.csv
          CONCURRENT_REQUESTS: 3
        run: |
          echo "Starting crawler..."
          python crawler/crawl.py
          echo "Crawling completed. Results:"
          [ -f data/results.csv ] && wc -l data/results.csv || echo "No results file found"

      # 7. Ergebnisse als Artefakt speichern
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: crawler-results
          path: |
            data/results.csv
          if-no-files-found: error
