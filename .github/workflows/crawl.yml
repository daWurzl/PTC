name: Web Crawler Schedule

on:
  schedule:
    - cron: "0 */4 * * *"
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Erhöhtes Timeout

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install aiohttp beautifulsoup4 pydantic python-dotenv playwright

      - name: Install Playwright with dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libpango-1.0-0 libcairo2
          playwright install chromium

      - name: Validate environment
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents:"
          ls -R
          echo "Checking .env:"
          cat crawler/.env || echo "No .env file found"

      - name: Create data directory
        run: |
          mkdir -p data
          sudo chmod -R 777 data  # Temporäre Berechtigungslösung

      - name: Run crawler with debug
        run: |
          python -u crawler/crawl.py 2>&1 | tee crawler.log
        env:
          PYTHONUNBUFFERED: 1

      - name: Upload logs
        if: always()  # Läuft auch bei Fehlern
        uses: actions/upload-artifact@v3
        with:
          name: debug-logs
          path: |
            crawler.log
            data/results.csv
